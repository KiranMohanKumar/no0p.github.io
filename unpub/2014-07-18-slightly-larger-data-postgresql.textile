---
layout: post
title: Largeish Data in Postgresql
categories: [postgresql]
---

h1. {{ page.title }}

p(meta). July 18, 2014 - Portland, OR

This post will explore some tips for working with slightly larger data sets in Postgresql while at home and on a budget.  Topics include table partitioning, file system compression, RAID0 configuration, and pgbench.

h2. big daÂ·ta

According to the internet, big datas are "data sets that are too large and complex to manipulate or interrogate with standard methods or tools."  Below, in a proof of excessive free time, is a diagram outlining my opinions regarding select points on the big data scale.

<center>
  <img class="imagedropshadow" src="/images/bigdata.png" style="width: 800px"/>
</center>

I'm currently working on a project that stores a data set roughly equivalent to the <a href="http://en.wikipedia.org/wiki/Wikipedia:Database_download">wikipedia article dumps</a>, about 5TB or so of text data.  To me, this is big data because it required driving to Fry's.

h2.  Disks and Filesystem

The storage required is readily available for a couple hundred bucks -- in the form of two inexpensive 3TB drives.  In a <a href="http://en.wikipedia.org/wiki/Standard_RAID_levels">RAID0 striped volume</a> this will provide roughly 6TB of storage.

<center>
<img class="imagedropshadow" src="/images/disks.png" width="600px"/>
</center>

Creating and mounting the filesystem is trivial.

bc.. sudo mkfs.btrfs -f -d raid0 /dev/sdc /dev/sdb
sudo mount -o compress=zlib /dev/sdb /storage

p.  I've chosen btrfs because I can no longer be held back by labels like "production ready."

bc.. $ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1       134G   78G   50G  62% /
/dev/sdb        5.5T  1.0M  5.5T   1% /storage

h2.  Postgresql

There are many considerations when configuring storage for Postgresql.  Consider the ways in which Postgresql writes to disk.  There are WAL files, the commit log, system logs, temporary statistics, and data pages.  The example below simple arranges to store data pages for a specific database on the larger volume.

Storage locations are handled by <a href="http://www.postgresql.org/docs/9.3/static/manage-ag-tablespaces.html">tablespaces</a> in Postgresql.  One gotcha is that the postgres user must own the directory (and all parent directories).

bc.. $ sudo chown -R postgres:postgres /storage/
$ psql -d template1
#> create tablespace bigdata location '/storage';

p. 

h2.  Benchmarking

There are a lot of possible permutations of configuration options.  When setting up a new system, such as additional storage, it pays to do at least some minimal benchmarking to make slightly more informed decisions.  Here we'll look at how the decision to use btrfs with compression was made by using pgbench and the excellent tooling that is <a href="https://github.com/gregs1104/pgbench-tools">pgbench-tools</a>.


bc.. #> \l+  
                              List of databases
         Name          |  Owner  | Tablespace |                Description                 
-----------------------+---------+------------+-------------------------------------------
 pgbench               | robert  | bigdata    | 
 postgres              | postgres| pg_default | default administrative connection database


p. 

<img src="/images/scaling_fs.png"/>

